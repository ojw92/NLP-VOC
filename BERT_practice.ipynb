{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ojw92/NLP-for-Text-Classification/blob/main/BERT_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "d2b3fe256b094e82a87b624b50b7af86",
        "deepnote_cell_type": "markdown",
        "id": "_u716cCIZIqP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8M_IjgNzPa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Practice\n",
        "#### v. 20230118"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "acbdd5e51fa346b2b141d7dcf4ba0274",
        "deepnote_cell_type": "markdown",
        "id": "JRkT4Q-yZIqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip3 install torch torchvision torchaudio     # for PyTorch without GPU, just CPU\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "d680c6cc343e44dda5236360ff9ce245",
        "source_hash": "a40f784d",
        "execution_start": 1674080120693,
        "execution_millis": 11,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "vKI5w-BMZIqc"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "id": "I7P3DUOpgV7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f79714f-4fed-49bd-be1a-0dbf0f1fc150"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the GPU can be detected\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()              # '/device:GPU:0' means GPU is enabled\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "8eWAtNCmrzso",
        "outputId": "476f2a5c-fbc4-47b3-9fd4-3503080851d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f5852a60dd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# '/device:GPU:0' means GPU is enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# torch.cuda.is_available = lambda : False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "1KyaineWiqgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo            # check memory resources available"
      ],
      "metadata": {
        "id": "Nf6uQarosHGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qu9kfdGl1Vv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCmAhc2i0JDh"
      },
      "outputs": [],
      "source": [
        "# install wandb for tracking data on dashboard\n",
        "!pip install datasets wandb evaluate -qU\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGvsX_9j0JDj"
      },
      "outputs": [],
      "source": [
        "# the run_glue.py script requires transformers dev\n",
        "!pip install -q git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of5C4XID0JDk"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# log in to have data synced to account\n",
        "wandb.login()\n",
        "\n",
        "# log every trained model\n",
        "%env WANDB_LOG_MODEL=true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/PradipNichite/Youtube-Tutorials/blob/main/FineTune_BERT_Model_Youtube.ipynb\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# filter training data to desired dates (after 2022.2.22)\n",
        "df = pd.read_csv('/S22_total.csv', index_col=0).drop_duplicates()\n",
        "\n",
        "# filter training data to desired dates\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "df = df[df.Date.apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').date() > datetime(2022,2,21).date())]\n",
        "\n",
        "# For class imbalance, use roughly same ratio of R & N\n",
        "# BERT doesn't need class imbalance addressed\n",
        "df = pd.concat([df[df.Class=='R'], df[df.Class=='N'].iloc[::3, :]])\n",
        "\n",
        "# Reduce data to reduce training time to test features\n",
        "# df22 = df22.sample(frac=0.01, random_state=5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "9f955299a3e54a1aaebfc738a20d1320",
        "source_hash": "76f0006d",
        "execution_start": 1674080120705,
        "execution_millis": 808,
        "is_output_hidden": false,
        "deepnote_table_state": {
          "sortBy": [],
          "filters": [],
          "pageSize": 10,
          "pageIndex": 0
        },
        "deepnote_table_loading": false,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "k6xcfS07ZIqd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Title + Content concatenator\n",
        "from timeit import default_timer as timer\n",
        "import re\n",
        "\n",
        "def data_concat(df22, R_known=True):\n",
        "\n",
        "    # Split the input dataframe into Text (Title + Contents) and Classes dataframes\n",
        "    # input must have 3 columns of string entries (Title, Contents, and Classes)\n",
        "\n",
        "    # check if every row entry of each column is string type (some are NaN, so no)\n",
        "    # print(df22.applymap(lambda x : type(x).__name__).eq({'Title': 'str', 'Content': 'str', 'Class':'str'}))\n",
        "\n",
        "    # convert NaN to empty strings (NaN -> str)\n",
        "        # df22.apply(str) converts all columns to str, as well\n",
        "    df22 = df22.replace(float('nan'), '', regex=True)\n",
        "\n",
        "    # concatenate strings of title & content with a \" \" in between (1 body of text)\n",
        "    df22['Text'] = df22['Title'] + \" \" + df22['Content']      # slicing DataFrame via .iloc[:,0] makes it a Series\n",
        "    df22 = df22.loc[: , ['Text', 'Class']]    # so initialize it as a DataFrame. pd.DataFrame(some_Series) works\n",
        "\n",
        "    if R_known == True:\n",
        "    # R, r, YR = 1;     N, n, YN = 0\n",
        "        R_cases = re.compile('R|YR', re.IGNORECASE)\n",
        "        N_cases = re.compile('N|YN', re.IGNORECASE)\n",
        "        df22['Class'] = df22['Class'].replace(to_replace=R_cases, value=1)\n",
        "        df22['Class'] = df22['Class'].replace(to_replace=N_cases, value=0)\n",
        "    else:\n",
        "        # R_known == False; prepping not yet classified data\n",
        "        Y_N_cases = re.compile('Y|N', re.IGNORECASE)\n",
        "        df22['Class'] = df22['Class'].replace(to_replace=Y_N_cases, value=0)     # all N's for simplicity\n",
        "\n",
        "    df22['Class'] = df22['Class'].astype('int32')\n",
        "\n",
        "\n",
        "    return df22\n"
      ],
      "metadata": {
        "id": "I94k-OsHcdmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine title & content as text22, clean the text, then combine it with labels to a single df  \n",
        "df = data_concat(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Nh1gY_I2c-aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "ddbfd30c829a40c9af60fd8fc4ad8bb3",
        "source_hash": "fda411fd",
        "execution_start": 1674080121523,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "kJtMeTRwZIqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "d968ff4c82ea4fe89fdeecd8ad261bda",
        "source_hash": "65934339",
        "execution_start": 1674080121543,
        "execution_millis": 5340,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "jk_eN8Q0ZIqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "506970be01814296ad84c7c9f3312165",
        "source_hash": "d991cab6",
        "execution_start": 1674080126876,
        "execution_millis": 88,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "FYxeRDLYZIqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to( torch.device('cuda') )     # need NVIDIA driver for 'cuda'; currently have AMD on work laptop\n",
        "# model = model.to('cpu')         # train on CPU\n",
        "\n",
        "model = model.to('cuda')          # or  model.cuda()\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "abaddecf0f7a4867b3088265ded4cec5",
        "source_hash": "ff8dd7cb",
        "execution_start": 1674080126980,
        "execution_millis": 1,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "65yDwWirZIqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data = [\"This is possibly the worst battery I have ever seen on a mobile device\",\n",
        "            \"How is my device running so smoothly?\"]\n",
        "tokenizer(test_data, padding=True, truncation=True, max_length=512)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "cbadffae0dd44bbbb994e70f20146772",
        "source_hash": "6c516043",
        "execution_start": 1674080126981,
        "execution_millis": 100,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "-z6XRhJgZIqg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(df[\"Text\"])\n",
        "y = list(df[\"Class\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "af668836fd5341ac9016a845ebe79057",
        "source_hash": "25d321c9",
        "execution_start": 1674080127028,
        "execution_millis": 27370,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "o-G23j4EZIqg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized.keys()"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "0ddc2b86e99d47f49d560a7a4a7427a9",
        "source_hash": "882ded45",
        "execution_start": 1674080154402,
        "execution_millis": 14,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "O2IkScN2ZIqh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tokenized['attention_mask'][0])"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "31306b38d6424983b8ec35e35b89d5d5",
        "source_hash": "c5b8c3f4",
        "execution_start": 1674080169475,
        "execution_millis": 17,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "yn6O2T8lZIqh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train),len(X_val)\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "c20d101dd6e84dff850d5446d87227b5",
        "source_hash": "1851768b",
        "execution_start": 1674080189480,
        "execution_millis": 0,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "WPVhi1tNZIqi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create torch dataset\n",
        "class VOC_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "785f8f506f724850be7cdd073ff8bfd4",
        "source_hash": "3b060da6",
        "execution_start": 1674080232215,
        "execution_millis": 12,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "OC-jBK8SZIqi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = VOC_Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = VOC_Dataset(X_val_tokenized, y_val)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "9b6f589cb6274e459cf4b21ca9416ac7",
        "source_hash": "78f426a3",
        "execution_start": 1674080252981,
        "execution_millis": 1,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "0J6cyoT6ZIqi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[5]\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "e74bd73ccb5e49869e5db0dc8d5da4db",
        "source_hash": "a1b8da62",
        "execution_start": 1674080260113,
        "execution_millis": 47,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "WT0-A0GyZIqj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(m):\n",
        "    print(type(m))\n",
        "    pred, labels = m\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "     "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "d89ebde51e854f5c9bfb45c57a0b8eb2",
        "source_hash": "2986a0e5",
        "execution_start": 1674080319332,
        "execution_millis": 17,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "ItJjuFt_ZIqj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "args = TrainingArguments(\n",
        "    report_to = 'wandb',                     # enable logging to W&B\n",
        "    output_dir=\"output\",                     # output directory\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8\n",
        "    # overwrite_output_dir = True,\n",
        "    # evaluation_strategy = 'steps',          # check evaluation metrics at each epoch\n",
        "    # learning_rate = 5e-5,                   # we can customize learning rate\n",
        "    # max_steps = 30000,\n",
        "    # logging_steps = 100,                    # we will log every 100 steps\n",
        "    # eval_steps = 5000,                      # we will perform evaluation every 500 steps\n",
        "    # save_steps = 10000,\n",
        "    # load_best_model_at_end = True,\n",
        "    # metric_for_best_model = 'accuracy',\n",
        "    # run_name = 'custom_training'            # name of the W&B run\n",
        "\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,            # for padding batched data\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "f95f543b3daa425ebecc01270b25bf91",
        "source_hash": "223c3ba0",
        "execution_start": 1674080330040,
        "execution_millis": 145,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "CFYxNIctZIqj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/facebookresearch/fairseq/issues/1933\n",
        "# https://huggingface.co/spaces/OFA-Sys/OFA-Generic_Interface/blob/8fc1d8aafce5821301443744696303bac6227f52/fairseq/examples/roberta/commonsense_qa/README.md\n",
        "\n",
        "# PyTorch fairseq\n",
        "\n",
        "MAX_UPDATES=2300      # Number of training steps, where   training step = training set / batch size. Originally 3000\n",
        "WARMUP_UPDATES=150    # Linearly increase LR over this many steps\n",
        "MAX_EPOCH=2           # Number of training epochs.\n",
        "LR=1e-06              # Peak LR for fixed LR scheduler. 1e-05 default; try 1e-06 as initial LR\n",
        "NUM_CLASSES=2\n",
        "MAX_SENTENCES=8       # Batch size per GPU.\n",
        "UPDATE_FREQ=32        # Accumulate gradients to simulate training on 8 GPUs. \n",
        "DATA_DIR='VOC_final_output'\n",
        "ROBERTA_PATH='/SiERoBERT_large/model.pt'\n",
        "\n",
        "! CUDA_VISIBLE_DEVICES=0 fairseq-train $DATA_DIR --ddp-backend=no_c10d \\\n",
        "  --restore-file $ROBERTA_PATH \\\n",
        "  --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "  --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\n",
        "  #--best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "  --task sentence_ranking \\\n",
        "  --num-classes $NUM_CLASSES \\\n",
        "  --init-token 0 --separator-token 2 \\\n",
        "  --max-option-length 128 \\\n",
        "  --max-positions 512 \\\n",
        "  --truncate-sequence \\\n",
        "  --arch roberta_large \\\n",
        "  --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
        "  --criterion sentence_ranking \\\n",
        "  --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \\\n",
        "  --clip-norm 0.0 \\\n",
        "  --lr-scheduler fixed --lr $LR \\\n",
        "  --warmup-updates $WARMUP_UPDATES --total-num-update $MAX_UPDATES \\\n",
        "  --memory-efficient-fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "  --batch-size $MAX_SENTENCES \\\n",
        "  --required-batch-size-multiple 1 \\\n",
        "  --update-freq $UPDATE_FREQ \\\n",
        "  --max-epoch $MAX_EPOCH \\\n",
        "  --log-interval 100 \\\n",
        "\n",
        "# --memory-efficient-fp16 instead of --fp16 solves 'CUDA out of memory' problem, but slow training\n",
        "# --max-sentences $MAX_SENTENCES , not batch-size\n",
        "# --fp16-scale-window 128 default"
      ],
      "metadata": {
        "id": "8gGKlopBdNYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "start = timer()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "end = timer()\n",
        "print(\"%4f seconds, %4f minutes elapsed\" % (float(end-start), float((end-start)/60)))"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "8185c9ee39774fcba9200e83819fce3f",
        "source_hash": "c4a351e7",
        "execution_start": 1674080563381,
        "execution_millis": 17834,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "8NO7NW3bZIqk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "97a062b8225e4125971486d915e3ac86",
        "deepnote_cell_type": "code",
        "id": "4L3oPcUvZIqk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "2920442657d045288b468403b8e7ce45",
        "deepnote_cell_type": "code",
        "id": "ee-3alVdZIqk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Super charging is working very well.\"\n",
        "# text = \"so many issues with this phone.\"\n",
        "inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)    # also gelu(), silu()\n",
        "print(predictions)\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "predictions"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "b95d7749d3074a20bdfa3cdc432b149e",
        "deepnote_cell_type": "code",
        "id": "Ndgwa_feZIql"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('BertPractice')"
      ],
      "metadata": {
        "id": "ehJCv_jKWoge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model('/content/drive/MyDrive/Youtube Tutorials/toxic')\n",
        "# model_2 = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Youtube Tutorials/toxic\")\n",
        "# model_2.to('cuda')"
      ],
      "metadata": {
        "id": "FDIfAtXeWqBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = BertForSequenceClassification.from_pretrained('BertPractice')\n",
        "model_2.to('cuda')"
      ],
      "metadata": {
        "id": "9af__siKWrvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Super charging is working very well.\"\n",
        "text = \"so many issues with this phone.\"\n",
        "inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
        "outputs = model_2(**inputs)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "predictions"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "53e74eee144d445da85927276d7daff3",
        "deepnote_cell_type": "code",
        "id": "Nf4WU--hZIql"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#today's VOC\n",
        "# import file and combine title & content\n",
        "voctoday = pd.read_csv('/20230124_test.csv', index_col=0).drop_duplicates()\n",
        "voctoday = data_concat(voctoday.loc[: , 'Title':'Class'])\n",
        "\n",
        "voclist = list(voctoday['Text'])\n",
        "predlist = []\n",
        "\n",
        "for i in range(len(voclist)):\n",
        "  inputs = tokenizer(voclist[i], padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
        "  outputs = model_2(**inputs)\n",
        "  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "  predictions = predictions.cpu().detach().numpy()\n",
        "  predlist.append(predictions)\n",
        "\n",
        "organizedzip = zip([x[:25] for x in voclist],\n",
        "                   predlist,\n",
        "                   voctoday['Class'])\n",
        "\n",
        "# results as voc text, predictions, and actual value\n",
        "vocresult = pd.DataFrame(list(organizedzip), columns=['Text','Prediction','Actual'])\n",
        "vocresult\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "6093bde4694f46c195c95d0798e529cb",
        "deepnote_cell_type": "code",
        "id": "IvNon4aUZIql"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "8207dc085d8b42718086312e08437452",
        "deepnote_cell_type": "code",
        "id": "5Bw4LKQzZIqm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "5aeac4c1c9b34e4b9afe24f40e1d3c27",
        "source_hash": "24acd7c3",
        "is_code_hidden": true,
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code",
        "id": "XHyZQQDLZIqm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "501fe0946abe4e6398eb6a364a652e3c",
        "source_hash": "b623e53d",
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code",
        "id": "HOzIXC_iZIqm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e5caba9f-cd36-4d50-aaa3-2cf59957a2f4' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "llB-FWxEZIqn"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "99baf882d48d4fa4b40b67d8bd9057bf",
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}